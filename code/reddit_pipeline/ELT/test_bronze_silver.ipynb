{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073e8289-6cd9-4117-8a05-cb244995d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os, io, json, gzip, re\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pyarrow as pa, pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709d5a05-f585-4273-8c33-f23724b5dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "# ROOT = Path(__file__).resolve().parents[1]\n",
    "ROOT = Path('/home/ubuntu/deds2025b_proj/opt/reddit_pipeline')    # FOR NOTEBOOK ONLY\n",
    "load_dotenv(ROOT / '.env')\n",
    "\n",
    "BUCKET = os.environ[\"LAKE_BUCKET\"]\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12b747c-3d64-4c5f-8df9-16bff10ea3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helper functions ---\n",
    "def list_keys(prefix):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):\n",
    "        for o in page.get(\"Contents\", []):\n",
    "            yield o[\"Key\"]\n",
    "\n",
    "def read_jsonl_gz(key):\n",
    "    buf = io.BytesIO()\n",
    "    s3.download_fileobj(BUCKET, key, buf)\n",
    "    buf.seek(0)\n",
    "    rows=[]\n",
    "    with gzip.GzipFile(fileobj=buf, mode=\"rb\") as gz:\n",
    "        for line in gz:\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def write_parquet(df, key):\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    out = io.BytesIO()\n",
    "    pq.write_table(table, out, compression=\"snappy\")\n",
    "    out.seek(0)\n",
    "    s3.upload_fileobj(out, BUCKET, key)\n",
    "\n",
    "def bronze_to_silver(date_str):\n",
    "    bronzepfx = f\"bronze/reddit/dt={date_str}/\"\n",
    "    post_frames, comment_frames, author_rows, subr_rows = [], [], [], []\n",
    "\n",
    "    for key in list_keys(bronzepfx):\n",
    "        m = re.search(r\"dt=(.+?)/subreddit=([^/]+)/run_id=([^/]+)/(.+)\\.jsonl\\.gz$\", key)\n",
    "        if not m:\n",
    "            continue\n",
    "        dt_part, sr, run_id, kind = m.groups()\n",
    "        rows = read_jsonl_gz(key)\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        if \"posts\" in kind:\n",
    "            df = pd.DataFrame(rows)\n",
    "            df[\"created_ts\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\", utc=True)\n",
    "            df[\"dt\"] = dt_part\n",
    "            df[\"subreddit\"] = sr\n",
    "            df[\"run_id\"] = run_id\n",
    "            \n",
    "            # dedup by post_name (keep last)\n",
    "            df = df.drop_duplicates(subset=[\"post_name\"], keep=\"last\")\n",
    "\n",
    "            post_frames.append(df[[\n",
    "                \"post_name\",\"subreddit_id\",\"author_fullname\",\"title\",\"selftext\",\"score\",\"subreddit\",\n",
    "                \"upvote_ratio\",\"num_comments\",\"url\",\"created_utc\",\"created_ts\",\"run_id\",\"dt\",\n",
    "                \"subreddit_name_prefixed\",\"subreddit_type\",\"subreddit_subscribers\",\"author\",\"author_premium\"\n",
    "            ]])\n",
    "\n",
    "            # dimension snapshots\n",
    "            subr_rows += df[[\"subreddit_id\",\"subreddit_name_prefixed\",\"subreddit_type\",\"subreddit_subscribers\"]].to_dict(\"records\")\n",
    "            \n",
    "            # author can be null\n",
    "            author_rows += df[[\"author_fullname\",\"author\",\"author_premium\"]].to_dict(\"records\")\n",
    "\n",
    "        elif \"comments\" in kind:\n",
    "            df = pd.DataFrame(rows)\n",
    "        \n",
    "            # normalize parent: set to NULL if parent is a post (t3_)\n",
    "            df[\"parent_comment_name\"] = df[\"parent_comment_name\"].where(~df[\"parent_comment_name\"].astype(str).str.startswith(\"t3_\"), None)\n",
    "            \n",
    "            df[\"created_ts\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\", utc=True)\n",
    "            df[\"dt\"] = dt_part\n",
    "            df[\"subreddit\"] = sr\n",
    "            df[\"run_id\"] = run_id\n",
    "\n",
    "            # dedup\n",
    "            df = df.drop_duplicates(subset=[\"comment_name\"], keep=\"last\")\n",
    "\n",
    "            # drop bot comments\n",
    "            mask = df['body'].str.contains('I am a bot', case=False, na=False)\n",
    "            df = df.loc[~mask]\n",
    "            \n",
    "            comment_frames.append(df[[\n",
    "                \"comment_name\",\"post_name\",\"parent_comment_name\",\"author_fullname\",\"subreddit\",\n",
    "                \"body\",\"score\",\"created_utc\",\"created_ts\",\"run_id\",\"dt\",\"author\",\"author_premium\"\n",
    "            ]])\n",
    "            \n",
    "            author_rows += df[[\"author_fullname\",\"author\",\"author_premium\"]].to_dict(\"records\")\n",
    "\n",
    "    # Concatenate and write partitioned folders\n",
    "    if post_frames:\n",
    "        posts = pd.concat(post_frames, ignore_index=True)\n",
    "        # write per subreddit partition\n",
    "        for sr, df_sr in posts.groupby(\"subreddit\"):\n",
    "            key = f\"silver/reddit/posts/dt={date_str}/subreddit={sr}/part-0.parquet\"\n",
    "            write_parquet(df_sr.drop(columns=[\"subreddit\"]), key)\n",
    "    if comment_frames:\n",
    "        comments = pd.concat(comment_frames, ignore_index=True)\n",
    "        for sr, df_sr in comments.groupby(\"subreddit\"):\n",
    "            key = f\"silver/reddit/comments/dt={date_str}/subreddit={sr}/part-0.parquet\"\n",
    "            write_parquet(df_sr.drop(columns=[\"subreddit\"]), key)\n",
    "    if author_rows:\n",
    "        a = pd.DataFrame(author_rows).drop_duplicates(subset=[\"author_fullname\"], keep=\"last\")\n",
    "        key = f\"silver/reddit/authors/dt={date_str}/part-0.parquet\"\n",
    "        write_parquet(a, key)\n",
    "    if subr_rows:\n",
    "        s = pd.DataFrame(subr_rows).drop_duplicates(subset=[\"subreddit_id\"], keep=\"last\")\n",
    "        key = f\"silver/reddit/subreddits/dt={date_str}/part-0.parquet\"\n",
    "        write_parquet(s, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca94dcd-c664-4ad7-8538-c7a95a7c601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     date_str = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "#     bronze_to_silver(date_str)\n",
    "\n",
    "date_str = '2025-08-24'\n",
    "bronze_to_silver(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90dc9101-5d96-4f39-899f-ab4c74e6babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 'post', 'post_name': 't3_1mta8v2', 'title': 'If curing cancer and AIDS forever required sacrificing your sibling, how would you decide and could you live with the choice?', 'selftext': '', 'score': 4, 'upvote_ratio': 0.56, 'num_comments': 34, 'url': 'https://www.reddit.com/r/AskPH/comments/1mta8v2/if_curing_cancer_and_aids_forever_required/', 'created_utc': 1755485174.0, 'author_fullname': 't2_6ix447di', 'author': 'Accomplished-Yam-504', 'author_premium': False, 'subreddit_id': 't5_3a7odq', 'subreddit_name_prefixed': 'r/AskPH', 'subreddit_type': 'public', 'subreddit_subscribers': 411467}\n"
     ]
    }
   ],
   "source": [
    "date_str = '2025-08-24'\n",
    "bronzepfx = f\"bronze/reddit/dt={date_str}/\"\n",
    "post_frames, comment_frames, author_rows, subr_rows = [], [], [], []\n",
    "\n",
    "# for key in list_keys(bronzepfx):\n",
    "#     m = re.search(r\"dt=(.+?)/subreddit=([^/]+)/run_id=([^/]+)/(.+)\\.jsonl\\.gz$\", key)\n",
    "#     if not m:\n",
    "#         continue\n",
    "#     dt_part, sr, run_id, kind = m.groups()\n",
    "#     rows = read_jsonl_gz(key)\n",
    "#     if not rows:\n",
    "#         continue\n",
    "\n",
    "#     if \"posts\" in kind:\n",
    "#         df = pd.DataFrame(rows)\n",
    "for i in read_jsonl_gz('bronze/reddit/dt=2025-08-24/subreddit=AskPH/run_id=20250824T041150Z/posts.jsonl.gz'):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64288593-bce3-43a1-9ed0-c978d8ef3071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63a013-d3d7-46c6-aa7b-9022e3ceb54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
