{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d42e28a-12db-4521-bcb4-1e12895abf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, uuid\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import boto3\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import pyarrow as pa, pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5360367-4517-41a5-aad9-815fc23c3ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- config ---\n",
    "# ROOT = Path(__file__).resolve().parents[1]\n",
    "ROOT = Path(\"/home/ubuntu/deds2025b_proj/opt/reddit_pipeline\")    # FOR NOTEBOOK ONLY\n",
    "load_dotenv(ROOT / \".env\")\n",
    "\n",
    "BUCKET = os.environ[\"LAKE_BUCKET\"]\n",
    "REDSHIFT_SECRET_ARN = os.environ[\"REDDIT_OLAP_ARN\"]\n",
    "REDSHIFT_DB = os.environ[\"OLAP_DB\"]\n",
    "REDSHIFT_IAM_ROLE_ARN = os.environ[\"IAM_ROLE_ARN\"]\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "secrets = boto3.client(\"secretsmanager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2db5a5-65df-40c1-91f5-02a614bc28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helper functions ---\n",
    "def list_keys(prefix: str):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):\n",
    "        for o in page.get(\"Contents\", []):\n",
    "            yield o[\"Key\"]\n",
    "\n",
    "def read_parquet_to_df(key: str) -> pd.DataFrame:\n",
    "    buf = io.BytesIO()\n",
    "    s3.download_fileobj(BUCKET, key, buf)\n",
    "    buf.seek(0)\n",
    "    table = pq.read_table(buf)\n",
    "    return table.to_pandas()\n",
    "\n",
    "def write_parquet(df: pd.DataFrame, key: str):\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    out = io.BytesIO()\n",
    "    pq.write_table(table, out, compression=\"snappy\")\n",
    "    out.seek(0)\n",
    "    s3.upload_fileobj(out, BUCKET, key)\n",
    "\n",
    "def write_staging_parquet(df: pd.DataFrame, key_prefix: str) -> str:\n",
    "    if df.empty:\n",
    "        return None\n",
    "    key = f\"{key_prefix}/part-{uuid.uuid4().hex}.parquet\"\n",
    "    write_parquet(df, key)\n",
    "    return \"/\".join(key.split(\"/\")[:-1]) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc955d45-6229-4f7a-a8cb-5ab4c3d4c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Redshift ---\n",
    "def get_redshift_conn():\n",
    "    cfg = json.loads(secrets.get_secret_value(SecretId=REDSHIFT_SECRET_ARN)[\"SecretString\"])\n",
    "    return psycopg2.connect(\n",
    "        host=cfg[\"host\"],\n",
    "        port=cfg.get(\"port\", 5439),\n",
    "        user=cfg[\"username\"],\n",
    "        password=cfg[\"password\"],\n",
    "        dbname=REDSHIFT_DB,\n",
    "    )\n",
    "\n",
    "def copy_from_parquet(cur, table_name: str, s3_prefix: str):\n",
    "    sql = f\"\"\"\n",
    "        COPY {table_name}\n",
    "        FROM 's3://{BUCKET}/{s3_prefix}'\n",
    "        IAM_ROLE '{REDSHIFT_IAM_ROLE_ARN}'\n",
    "        FORMAT AS PARQUET;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17deefe-8da7-4413-81de-4705ff9a3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- tags staging ---\n",
    "def build_tag_staging(date_str: str) -> str | None:\n",
    "    posts_prefix    = f\"gold/reddit/post_enriched/dt={date_str}/\"\n",
    "    comments_prefix = f\"gold/reddit/comment_enriched/dt={date_str}/\"\n",
    "\n",
    "    # Build post_name -> subreddit_id map while reading posts\n",
    "    post_to_subr = {}\n",
    "    tag_rows = []\n",
    "\n",
    "    # POSTS\n",
    "    post_keys = [k for k in list_keys(posts_prefix) if k.endswith(\".parquet\")]\n",
    "    for key in post_keys:\n",
    "        df = read_parquet_to_df(key)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        # map post to subreddit_id\n",
    "        if \"post_name\" in df and \"subreddit_id\" in df:\n",
    "            post_to_subr.update(df.set_index(\"post_name\")[\"subreddit_id\"].to_dict())\n",
    "        if \"tags\" not in df.columns:\n",
    "            continue\n",
    "        # explode tags\n",
    "        tmp = df[[\"tags\", \"created_ts\", \"subreddit_id\", \"score\", \"negative_s\", \"neutral_s\", \"positive_s\"]].copy()\n",
    "        tmp = tmp.explode(\"tags\").dropna(subset=[\"tags\"])\n",
    "        if tmp.empty:\n",
    "            continue\n",
    "        tmp[\"date_value\"] = pd.to_datetime(tmp[\"created_ts\"], utc=True).dt.date\n",
    "        tmp = tmp.rename(columns={\n",
    "            \"tags\": \"tag_name\",\n",
    "            \"negative_s\": \"negative\",\n",
    "            \"neutral_s\": \"neutral\",\n",
    "            \"positive_s\": \"positive\",\n",
    "        })\n",
    "        tag_rows.append(tmp[[\"tag_name\", \"date_value\", \"subreddit_id\", \"score\", \"negative\", \"neutral\", \"positive\"]])\n",
    "\n",
    "    # COMMENTS\n",
    "    comment_keys = [k for k in list_keys(comments_prefix) if k.endswith(\".parquet\")]\n",
    "    for key in comment_keys:\n",
    "        df = read_parquet_to_df(key)\n",
    "        if df.empty or \"tags\" not in df.columns:\n",
    "            continue\n",
    "        # ensure subreddit_id exists for comments via post map\n",
    "        if \"subreddit_id\" not in df.columns and \"post_name\" in df.columns:\n",
    "            df[\"subreddit_id\"] = df[\"post_name\"].map(post_to_subr)\n",
    "        tmp = df[[\"tags\", \"created_ts\", \"subreddit_id\", \"score\", \"negative_s\", \"neutral_s\", \"positive_s\"]].copy()\n",
    "        tmp = tmp.explode(\"tags\").dropna(subset=[\"tags\", \"subreddit_id\"])\n",
    "        if tmp.empty:\n",
    "            continue\n",
    "        tmp[\"date_value\"] = pd.to_datetime(tmp[\"created_ts\"], utc=True).dt.date\n",
    "        tmp = tmp.rename(columns={\n",
    "            \"tags\": \"tag_name\",\n",
    "            \"negative_s\": \"negative\",\n",
    "            \"neutral_s\": \"neutral\",\n",
    "            \"positive_s\": \"positive\",\n",
    "        })\n",
    "        tag_rows.append(tmp[[\"tag_name\", \"date_value\", \"subreddit_id\", \"score\", \"negative\", \"neutral\", \"positive\"]])\n",
    "\n",
    "    if not tag_rows:\n",
    "        return None\n",
    "\n",
    "    stg = pd.concat(tag_rows, ignore_index=True)\n",
    "    return write_staging_parquet(stg, f\"tmp/redshift/stg_tag_rows/dt={date_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d5e42-4b44-470c-8bc8-1fcf7b0d9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- main ---\n",
    "def load_olap_for_date(date_str: str):\n",
    "    posts_prefix    = f\"gold/reddit/post_enriched/dt={date_str}/\"\n",
    "    comments_prefix = f\"gold/reddit/comment_enriched/dt={date_str}/\"\n",
    "\n",
    "    has_posts = any(list_keys(posts_prefix))\n",
    "    has_comments = any(list_keys(comments_prefix))\n",
    "    if not has_posts and not has_comments:\n",
    "        print(f\"[{date_str}] No Gold data found under {posts_prefix} or {comments_prefix}. Nothing to load.\")\n",
    "        return\n",
    "\n",
    "    # Build tag staging\n",
    "    stg_tags_pfx = build_tag_staging(date_str)\n",
    "\n",
    "    conn = get_redshift_conn()\n",
    "    try:\n",
    "        with conn, conn.cursor() as cur:\n",
    "            # -------- staging tables --------\n",
    "            if has_posts:\n",
    "                cur.execute(\"\"\"\n",
    "                    CREATE TEMP TABLE stg_post_enriched (\n",
    "                      post_name VARCHAR(32),\n",
    "                      subreddit_id VARCHAR(32),\n",
    "                      author_fullname VARCHAR(32),\n",
    "                      title VARCHAR(300),\n",
    "                      selftext TEXT,\n",
    "                      score INT,\n",
    "                      upvote_ratio DECIMAL(5, 4),\n",
    "                      num_comments INTEGER,\n",
    "                      url TEXT,\n",
    "                      created_utc BIGINT,\n",
    "                      created_ts TIMESTAMPTZ,\n",
    "                      run_id VARCHAR(32),\n",
    "                      dt DATE,\n",
    "                      subreddit_name_prefixed VARCHAR(100),\n",
    "                      subreddit_type VARCHAR(20),\n",
    "                      subreddit_subscribers BIGINT,\n",
    "                      author VARCHAR(100),\n",
    "                      author_premium VARCHAR(20),\n",
    "                      tags SUPER,\n",
    "                      negative_s DECIMAL(6, 4),\n",
    "                      neutral_s DECIMAL(6, 4),\n",
    "                      positive_s DECIMAL(6, 4),\n",
    "                      sentiment_label VARCHAR(16),\n",
    "                      scored_at TIMESTAMPTZ\n",
    "                    );\n",
    "                \"\"\")\n",
    "                copy_from_parquet(cur, \"stg_post_enriched\", posts_prefix)\n",
    "\n",
    "            if has_comments:\n",
    "                cur.execute(\"\"\"\n",
    "                    CREATE TEMP TABLE stg_comment_enriched (\n",
    "                      comment_name     VARCHAR(32),\n",
    "                      post_name        VARCHAR(32),\n",
    "                      author_fullname  VARCHAR(32),\n",
    "                      author           VARCHAR(100),\n",
    "                      author_premium   VARCHAR(20),\n",
    "                      score            INT,\n",
    "                      created_ts       TIMESTAMP,\n",
    "                      negative_s       FLOAT4,\n",
    "                      neutral_s        FLOAT4,\n",
    "                      positive_s       FLOAT4\n",
    "                    );\n",
    "                \"\"\")\n",
    "                copy_from_parquet(cur, \"stg_comment_enriched\", comments_prefix)\n",
    "\n",
    "            if stg_tags_pfx:\n",
    "                cur.execute(\"\"\"\n",
    "                    CREATE TEMP TABLE stg_tag_rows (\n",
    "                      tag_name     VARCHAR(100),\n",
    "                      date_value   DATE,\n",
    "                      subreddit_id VARCHAR(32),\n",
    "                      score        INT,\n",
    "                      negative     FLOAT4,\n",
    "                      neutral      FLOAT4,\n",
    "                      positive     FLOAT4\n",
    "                    );\n",
    "                \"\"\")\n",
    "                copy_from_parquet(cur, \"stg_tag_rows\", stg_tags_pfx)\n",
    "\n",
    "            # -------- DIMS --------\n",
    "            # dim_date from any created_ts we have\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO dim_date (date_value, \"year\",\"month\",\"day\",\"dow\",month_name,dow_name,is_weekend)\n",
    "                SELECT d, EXTRACT(year FROM d)::SMALLINT, EXTRACT(month FROM d)::SMALLINT,\n",
    "                       EXTRACT(day FROM d)::SMALLINT, EXTRACT(dow FROM d)::SMALLINT,\n",
    "                       TO_CHAR(d,'Month'), TO_CHAR(d,'Dy'),\n",
    "                       CASE WHEN EXTRACT(dow FROM d) IN (0,6) THEN 'Weekend' ELSE 'Weekday' END\n",
    "                FROM (\n",
    "                  SELECT DISTINCT CAST(created_ts AS DATE) AS d FROM stg_post_enriched\n",
    "                  UNION\n",
    "                  SELECT DISTINCT CAST(created_ts AS DATE) FROM stg_comment_enriched\n",
    "                ) x\n",
    "                WHERE NOT EXISTS (SELECT 1 FROM dim_date dd WHERE dd.date_value = x.d);\n",
    "            \"\"\")\n",
    "\n",
    "            # dim_subreddit (posts are authoritative for metadata)\n",
    "            if has_posts:\n",
    "                cur.execute(\"\"\"\n",
    "                    MERGE INTO dim_subreddit t\n",
    "                    USING (\n",
    "                      SELECT DISTINCT subreddit_id, subreddit_name_prefixed, subreddit_type, subreddit_subscribers\n",
    "                      FROM stg_post_enriched\n",
    "                      WHERE subreddit_id IS NOT NULL\n",
    "                    ) s\n",
    "                    ON t.subreddit_id = s.subreddit_id\n",
    "                    WHEN MATCHED THEN UPDATE\n",
    "                      SET subreddit_name_prefixed = s.subreddit_name_prefixed,\n",
    "                          subreddit_type          = s.subreddit_type,\n",
    "                          subreddit_subscribers   = s.subreddit_subscribers\n",
    "                    WHEN NOT MATCHED THEN INSERT (subreddit_id, subreddit_name_prefixed, subreddit_type, subreddit_subscribers)\n",
    "                      VALUES (s.subreddit_id, s.subreddit_name_prefixed, s.subreddit_type, s.subreddit_subscribers);\n",
    "                \"\"\")\n",
    "\n",
    "            # dim_author from posts + comments\n",
    "            cur.execute(\"\"\"\n",
    "                MERGE INTO dim_author t\n",
    "                USING (\n",
    "                  SELECT DISTINCT author_fullname, author, author_premium\n",
    "                  FROM (\n",
    "                    SELECT author_fullname, author, author_premium FROM stg_post_enriched\n",
    "                    UNION ALL\n",
    "                    SELECT author_fullname, author, author_premium FROM stg_comment_enriched\n",
    "                  ) u\n",
    "                  WHERE author_fullname IS NOT NULL\n",
    "                ) s\n",
    "                ON t.author_fullname = s.author_fullname\n",
    "                WHEN MATCHED THEN UPDATE\n",
    "                  SET author = s.author, author_premium = s.author_premium\n",
    "                WHEN NOT MATCHED THEN INSERT (author_fullname, author, author_premium)\n",
    "                  VALUES (s.author_fullname, s.author, s.author_premium);\n",
    "            \"\"\")\n",
    "\n",
    "            # -------- FACTS --------\n",
    "            if has_posts:\n",
    "                cur.execute(\"\"\"\n",
    "                    MERGE INTO fact_post f\n",
    "                    USING (\n",
    "                      SELECT\n",
    "                        p.post_name,\n",
    "                        dd.date_key,\n",
    "                        ds.subreddit_sk,\n",
    "                        da.author_sk,\n",
    "                        p.score,\n",
    "                        p.upvote_ratio,\n",
    "                        p.num_comments,\n",
    "                        p.negative_s AS negative,\n",
    "                        p.neutral_s  AS neutral,\n",
    "                        p.positive_s AS positive,\n",
    "                        (p.positive_s - p.negative_s) AS net_sentiment\n",
    "                      FROM stg_post_enriched p\n",
    "                      JOIN dim_date dd        ON dd.date_value = CAST(p.created_ts AS DATE)\n",
    "                      JOIN dim_subreddit ds   ON ds.subreddit_id = p.subreddit_id\n",
    "                      LEFT JOIN dim_author da ON da.author_fullname = p.author_fullname\n",
    "                    ) s\n",
    "                    ON f.post_name = s.post_name\n",
    "                    WHEN MATCHED THEN UPDATE SET\n",
    "                      date_key = s.date_key,\n",
    "                      subreddit_sk = s.subreddit_sk,\n",
    "                      author_sk = s.author_sk,\n",
    "                      score = s.score,\n",
    "                      upvote_ratio = s.upvote_ratio,\n",
    "                      num_comments = s.num_comments,\n",
    "                      negative = s.negative,\n",
    "                      neutral = s.neutral,\n",
    "                      positive = s.positive,\n",
    "                      net_sentiment = s.net_sentiment\n",
    "                    WHEN NOT MATCHED THEN INSERT\n",
    "                      (post_name, date_key, subreddit_sk, author_sk, score, upvote_ratio, num_comments,\n",
    "                       negative, neutral, positive, net_sentiment)\n",
    "                    VALUES\n",
    "                      (s.post_name, s.date_key, s.subreddit_sk, s.author_sk, s.score, s.upvote_ratio, s.num_comments,\n",
    "                       s.negative, s.neutral, s.positive, s.net_sentiment);\n",
    "                \"\"\")\n",
    "\n",
    "            if has_comments:\n",
    "                cur.execute(\"\"\"\n",
    "                    MERGE INTO fact_comment f\n",
    "                    USING (\n",
    "                      SELECT\n",
    "                        c.comment_name,\n",
    "                        c.post_name,\n",
    "                        dd.date_key,\n",
    "                        fp.subreddit_sk,\n",
    "                        da.author_sk,\n",
    "                        c.score,\n",
    "                        c.negative_s AS negative,\n",
    "                        c.neutral_s  AS neutral,\n",
    "                        c.positive_s AS positive,\n",
    "                        (c.positive_s - c.negative_s) AS net_sentiment\n",
    "                      FROM stg_comment_enriched c\n",
    "                      JOIN dim_date dd        ON dd.date_value = CAST(c.created_ts AS DATE)\n",
    "                      LEFT JOIN fact_post fp  ON fp.post_name = c.post_name\n",
    "                      LEFT JOIN dim_author da ON da.author_fullname = c.author_fullname\n",
    "                    ) s\n",
    "                    ON f.comment_name = s.comment_name\n",
    "                    WHEN MATCHED THEN UPDATE SET\n",
    "                      post_name = s.post_name,\n",
    "                      date_key = s.date_key,\n",
    "                      subreddit_sk = s.subreddit_sk,\n",
    "                      author_sk = s.author_sk,\n",
    "                      score = s.score,\n",
    "                      negative = s.negative,\n",
    "                      neutral  = s.neutral,\n",
    "                      positive = s.positive,\n",
    "                      net_sentiment = s.net_sentiment\n",
    "                    WHEN NOT MATCHED THEN INSERT\n",
    "                      (comment_name, post_name, date_key, subreddit_sk, author_sk, score,\n",
    "                       negative, neutral, positive, net_sentiment)\n",
    "                    VALUES\n",
    "                      (s.comment_name, s.post_name, s.date_key, s.subreddit_sk, s.author_sk, s.score,\n",
    "                       s.negative, s.neutral, s.positive, s.net_sentiment);\n",
    "                \"\"\")\n",
    "\n",
    "            # -------- TAGS --------\n",
    "            if stg_tags_pfx:\n",
    "                cur.execute(\"\"\"\n",
    "                    MERGE INTO fact_tags t\n",
    "                    USING (\n",
    "                      SELECT\n",
    "                        r.tag_name,\n",
    "                        dd.date_key,\n",
    "                        ds.subreddit_sk,\n",
    "                        SUM(COALESCE(r.score,0))                AS score,\n",
    "                        AVG(r.negative)                          AS negative,\n",
    "                        AVG(r.neutral)                           AS neutral,\n",
    "                        AVG(r.positive)                          AS positive,\n",
    "                        AVG(r.positive - r.negative)             AS net_sentiment\n",
    "                      FROM stg_tag_rows r\n",
    "                      JOIN dim_date dd      ON dd.date_value = r.date_value\n",
    "                      JOIN dim_subreddit ds ON ds.subreddit_id = r.subreddit_id\n",
    "                      GROUP BY 1,2,3\n",
    "                    ) s\n",
    "                    ON t.tag_name = s.tag_name AND t.date_key = s.date_key AND t.subreddit_sk = s.subreddit_sk\n",
    "                    WHEN MATCHED THEN UPDATE SET\n",
    "                      score = s.score,\n",
    "                      negative = s.negative, neutral = s.neutral,\n",
    "                      positive = s.positive, net_sentiment = s.net_sentiment\n",
    "                    WHEN NOT MATCHED THEN INSERT\n",
    "                      (tag_name, date_key, subreddit_sk, score, negative, neutral, positive, net_sentiment)\n",
    "                    VALUES\n",
    "                      (s.tag_name, s.date_key, s.subreddit_sk, s.score, s.negative, s.neutral, s.positive, s.net_sentiment);\n",
    "                \"\"\")\n",
    "\n",
    "            cur.execute(\"ANALYZE;\")\n",
    "        print(f\"[{date_str}] OLAP load complete.\")\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    load_olap_for_date(args.dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
